Compile and Deploy Models with Amazon SageMaker Neo
Neo is a capability of Amazon SageMaker that enables machine learning models to train once and run anywhere in the cloud and at the edge.
Ordinarily, optimizing machine learning models for inference on multiple platforms is extremely difficult because you need to hand-tune models for the specific hardware and software configuration of each platform. If you want to get optimal performance for a given workload, you need to know the hardware architecture, instruction set, memory access patterns, and input data shapes among other factors. For traditional software development, tools such as compilers and profilers simplify the process. For machine learning, most tools are specific to the framework or to the hardware. This forces you into a manual trial-and-error process that is unreliable and unproductive.
Neo automatically optimizes Gluon, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, and ONNX models for inference on Android, Linux, and Windows machines based on processors from Ambarella, ARM, Intel, Nvidia, NXP, Qualcomm, Texas Instruments, and Xilinx. Neo is tested with computer vision models available in the model zoos across the frameworks. 
Neo can optimize models with parameters either in FP32 or quantized to INT8 or FP16 bit-width. Neo consists of a compiler and a runtime. First, the Neo compilation API reads models exported from various frameworks. It converts the framework-specific functions and operations into a framework-agnostic intermediate representation. Next, it performs a series of optimizations. Then it generates binary code for the optimized operations, writes them to a shared object library, and saves the model definition and parameters into separate files. Neo also provides a runtime for each target platform that loads and executes the compiled model.
You can create a Neo compilation job from either the SageMaker console, AWS Command Line Interface (AWS CLI), Python notebook, or the SageMaker SDK. With a few CLI commands, an API invocation, or a few clicks, you can convert a model for your chosen platform. You can deploy the model to an SageMaker endpoint or on an AWS IoT Greengrass device quickly. SageMaker provides Neo container images for SageMaker XGBoost and Image Classification models, and supports SageMaker-compatible containers for your own compiled models.
Neo Available Regions, Frameworks, and Operators
Neo Available Regions
Neo is available in the following AWS Service Regions where SageMaker is supported: 
 Asia Pacific (Hong Kong, Mumbai, Seoul, Singapore, Sydney, Tokyo)
 Canada (Central)
 China (Beijing, Ningxia)
 EU (Frankfurt, Ireland, London, Paris, Stockholm)
 Middle East (Bahrain)
 North America (N. Virginia, Ohio, Oregon, N. California)
 South America (Sao Paulo)
 AWS GovCloud (US-Gov-West)
Neo-supported Frameworks and Operators
The image classification model files need to be formatted as a tar file (tar.gz) that includes additional files that depend on the type of framework.
To find look-up tables of Neo-supported frameworks, operators, devices, and platforms, see the following pages:
 SageMaker Neo-supported Frameworks and Operators
 SageMaker Neo-supported Devices and Platforms
Neo supports the following deep learning frameworks:
 TensorFlow: Neo supports saved models and frozen models.
For saved models, Neo expects one PB (.pb) or one PBTXT (.pbtxt) file and a variables directory that contains variables. 
For frozen models, Neo expect only one PB (.pb) or PBTXT (.pbtxt) file.
 Keras: Neo expects one H5 file (.h5) containing the model definition.
 PyTorch: Neo expects one PTH file (.pth) containing the model definition.
 MXNET: Neo expects one symbol file (.json) and one parameter file (.params).
 XGBoost: Neo expects one XGBoost model file (.model) where the number of nodes in a tree can't exceed 2^31.
 ONNX: Neo expects one ONNX file (.onnx).
 TFLite: Neo expects one NHWC file (.tflite).
Topics
 Neo Available Regions, Frameworks, and Operators
 Neo Model Compilation Sample Notebooks
 Use Neo to Compile a Model
 Deploy a Model
 Request Inferences from a Deployed Service
 Troubleshooting Neo Compilation Errors
Neo Model Compilation Sample Notebooks
For sample notebooks that uses SageMaker Neo to train, compile, optimize, and deploy machine learning models to make inferences, see: 
 MNIST Training, Compilation and Deployment with MXNet Module
 MNIST Training, Compilation and Deployment with Tensorflow Module
 Deploying pre-trained PyTorch vision models with SageMaker Neo 
 Model Optimization with an Image Classification Example
 Model Optimization with XGBoost Example 
For instructions on how to run these example notebooks in SageMaker, see Example Notebooks. If you need instructions on how to create a notebook instance to run these examples, see SageMaker, see Use Amazon SageMaker Notebook Instances. To navigate to the relevant example in your notebook instance, choose the Amazon SageMaker Examples tab to see a list of all of the SageMaker samples. To open a notebook, choose its Use tab, then choose Create copy.