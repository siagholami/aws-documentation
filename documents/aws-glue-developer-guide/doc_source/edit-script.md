# Editing Scripts in AWS Glue<a name="edit-script"></a>

A script contains the code that extracts data from sources, transforms it, and loads it into targets\. AWS Glue runs a script when it starts a job\.

AWS Glue ETL scripts can be coded in Python or Scala\. Python scripts use a language that is an extension of the PySpark Python dialect for extract, transform, and load \(ETL\) jobs\. The script contains *extended constructs* to deal with ETL transformations\. When you automatically generate the source code logic for your job, a script is created\. You can edit this script, or you can provide your own script to process your ETL work\.

 For information about defining and editing scripts using the AWS Glue console, see [Working with Scripts on the AWS Glue Console](console-edit-script.md)\.

## Defining a Script<a name="script-defining"></a>

Given a source and target, AWS Glue can generate a script to transform the data\. This proposed script is an initial version that fills in your sources and targets, and suggests transformations in PySpark\. You can verify and modify the script to fit your business needs\. Use the script editor in AWS Glue to add arguments that specify the source and target, and any other arguments that are required to run\. Scripts are run by jobs, and jobs are started by triggers, which can be based on a schedule or an event\. For more information about triggers, see [Triggering Jobs in AWS Glue](trigger-job.md)\.

In the AWS Glue console, the script is represented as code\. You can also view the script as a diagram that uses annotations \(\#\#\) embedded in the script\.  These annotations describe the parameters, transform types, arguments, inputs, and other characteristics of the script that are used to generate a diagram in the AWS Glue console\.

The diagram of the script shows the following:
+ Source inputs to the script
+ Transforms 
+ Target outputs written by the script

Scripts can contain the following annotations:


| Annotation | Usage | 
| --- | --- | 
| @params | Parameters from the ETL job that the script requires\. | 
| @type | Type of node in the diagram, such as the transform type, data source, or data sink\. | 
| @args | Arguments passed to the node, except reference to input data\. | 
| @return | Variable returned from script\. | 
| @inputs | Data input to node\. | 

To learn about the code constructs within a script, see [Program AWS Glue ETL Scripts in Python](aws-glue-programming-python.md)\.

**Example**  
The following is an example of a script generated by AWS Glue\. The script is for a job that copies a simple dataset from one Amazon Simple Storage Service \(Amazon S3\) location to another, changing the format from CSV to JSON\. After some initialization code, the script includes commands that specify the data source, the mappings, and the target \(data sink\)\. Note also the annotations\.  

```
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
## @type: DataSource
## @args: [database = "sample-data", table_name = "taxi_trips", transformation_ctx = "datasource0"]
## @return: datasource0
## @inputs: []
datasource0 = glueContext.create_dynamic_frame.from_catalog(database = "sample-data", table_name = "taxi_trips", transformation_ctx = "datasource0")
## @type: ApplyMapping
## @args: [mapping = [("vendorid", "long", "vendorid", "long"), ("tpep_pickup_datetime", "string", "tpep_pickup_datetime", "string"), ("tpep_dropoff_datetime", "string", "tpep_dropoff_datetime", "string"), ("passenger_count", "long", "passenger_count", "long"), ("trip_distance", "double", "trip_distance", "double"), ("ratecodeid", "long", "ratecodeid", "long"), ("store_and_fwd_flag", "string", "store_and_fwd_flag", "string"), ("pulocationid", "long", "pulocationid", "long"), ("dolocationid", "long", "dolocationid", "long"), ("payment_type", "long", "payment_type", "long"), ("fare_amount", "double", "fare_amount", "double"), ("extra", "double", "extra", "double"), ("mta_tax", "double", "mta_tax", "double"), ("tip_amount", "double", "tip_amount", "double"), ("tolls_amount", "double", "tolls_amount", "double"), ("improvement_surcharge", "double", "improvement_surcharge", "double"), ("total_amount", "double", "total_amount", "double")], transformation_ctx = "applymapping1"]
## @return: applymapping1
## @inputs: [frame = datasource0]
applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("vendorid", "long", "vendorid", "long"), ("tpep_pickup_datetime", "string", "tpep_pickup_datetime", "string"), ("tpep_dropoff_datetime", "string", "tpep_dropoff_datetime", "string"), ("passenger_count", "long", "passenger_count", "long"), ("trip_distance", "double", "trip_distance", "double"), ("ratecodeid", "long", "ratecodeid", "long"), ("store_and_fwd_flag", "string", "store_and_fwd_flag", "string"), ("pulocationid", "long", "pulocationid", "long"), ("dolocationid", "long", "dolocationid", "long"), ("payment_type", "long", "payment_type", "long"), ("fare_amount", "double", "fare_amount", "double"), ("extra", "double", "extra", "double"), ("mta_tax", "double", "mta_tax", "double"), ("tip_amount", "double", "tip_amount", "double"), ("tolls_amount", "double", "tolls_amount", "double"), ("improvement_surcharge", "double", "improvement_surcharge", "double"), ("total_amount", "double", "total_amount", "double")], transformation_ctx = "applymapping1")
## @type: DataSink
## @args: [connection_type = "s3", connection_options = {"path": "s3://example-data-destination/taxi-data"}, format = "json", transformation_ctx = "datasink2"]
## @return: datasink2
## @inputs: [frame = applymapping1]
datasink2 = glueContext.write_dynamic_frame.from_options(frame = applymapping1, connection_type = "s3", connection_options = {"path": "s3://example-data-destination/taxi-data"}, format = "json", transformation_ctx = "datasink2")
job.commit()
```