Write a Spark Application
Spark applications can be written in Scala, Java, or Python. There are several examples of Spark applications located on Spark Examples topic in the Apache Spark documentation. The Estimating Pi example is shown below in the three natively supported applications. You can also view complete examples in $SPARK_HOME/examples and at GitHub. For more information about how to build JARs for Spark, see the Quick Start topic in the Apache Spark documentation.
Scala
```
package org.apache.spark.examples
import scala.math.random
import org.apache.spark._
/ Computes an approximation to pi /
object SparkPi {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("Spark Pi")
    val spark = new SparkContext(conf)
    val slices = if (args.length > 0) args(0).toInt else 2
    val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow
    val count = spark.parallelize(1 until n, slices).map { i =>
      val x = random * 2 - 1
      val y = random * 2 - 1
      if (xx  y*y < 1) 1 else 0
    }.reduce(  )
    println("Pi is roughly "  4.0 * count / n)
    spark.stop()
  }
}
```
Java
```
package org.apache.spark.examples;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import java.util.ArrayList;
import java.util.List;
/* 
 * Computes an approximation to pi
 * Usage: JavaSparkPi [slices]
 /
public final class JavaSparkPi {
public static void main(String[] args) throws Exception {
    SparkConf sparkConf = new SparkConf().setAppName("JavaSparkPi");
    JavaSparkContext jsc = new JavaSparkContext(sparkConf);
int slices = (args.length == 1) ? Integer.parseInt(args[0]) : 2;
int n = 100000 * slices;
List<Integer> l = new ArrayList<Integer>(n);
for (int i = 0; i < n; i) {
  l.add(i);
}

JavaRDD<Integer> dataSet = jsc.parallelize(l, slices);

int count = dataSet.map(new Function<Integer, Integer>() {
  @Override
  public Integer call(Integer integer) {
    double x = Math.random() * 2 - 1;
    double y = Math.random() * 2 - 1;
    return (x * x  y * y < 1) ? 1 : 0;
  }
}).reduce(new Function2<Integer, Integer, Integer>() {
  @Override
  public Integer call(Integer integer, Integer integer2) {
    return integer  integer2;
  }
});

System.out.println("Pi is roughly "  4.0 * count / n);

jsc.stop();

}
}
```
Python 2.7
```
import sys
from random import random
from operator import add
from pyspark import SparkContext
if name == "main":
    """
        Usage: pi [partitions]
    """
    sc = SparkContext(appName="PythonPi")
    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2
    n = 100000 * partitions
def f(_):
    x = random() * 2 - 1
    y = random() * 2 - 1
    return 1 if x ** 2  y ** 2 < 1 else 0

count = sc.parallelize(xrange(1, n  1), partitions).map(f).reduce(add)
print "Pi is roughly %f" % (4.0 * count / n)

sc.stop()

```