# Inference<a name="tutorial-inference"></a>

This section provides tutorials on how to run inference using the DLAMI's frameworks and tools\.

For tutorials using Elastic Inference, see [Working with Amazon Elastic Inference](https://docs.aws.amazon.com/elastic-inference/latest/developerguide/working-with-ei.html) 

## Inference with Frameworks<a name="tutorial-inference-frameworks"></a>
+ [Use Apache MXNet for Inference with an ONNX Model](tutorial-mxnet-inference-onnx.md)
+ [Use Apache MXNet for Inference with a ResNet 50 Model](tutorial-mxnet-inference-resnet50.md)
+ [Use CNTK for Inference with an ONNX Model](tutorial-cntk-inference-onnx.md)

## Inference Tools<a name="tutorial-inference-tools"></a>
+ [Model Server for Apache MXNet \(MMS\)](tutorial-mms.md)
+ [TensorFlow Serving](tutorial-tfserving.md)