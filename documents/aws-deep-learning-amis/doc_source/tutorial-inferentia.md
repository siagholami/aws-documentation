# The AWS Inferentia Chip With DLAMI<a name="tutorial-inferentia"></a>

AWS Inferentia is a custom machine learning chip designed by AWS that you can use for high\-performance inference predictions\. In order to use the chip, set up an Amazon Elastic Compute Cloud instance and use the AWS Neuron software development kit \(SDK\) to invoke the Inferentia chip\. To provide customers with the best Inferentia experience, Neuron has been built into the AWS Deep Learning AMI \(DLAMI\)\. 

The following topics show you how to get started using Inferentia with the DLAMI\. 

**Topics**
+ [Launching a DLAMI Instance with AWS Neuron](tutorial-inferentia-launching.md)
+ [Using the DLAMI with AWS Neuron](tutorial-inferentia-using.md)