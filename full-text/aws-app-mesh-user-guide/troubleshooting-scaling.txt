App Mesh scaling troubleshooting
This topic details common issues that you may experience with App Mesh scaling.
Connectivity fails and container health checks fail when scaling beyond 10 replicas for a virtual node
Symptoms
When you are scaling the number of replicas, such as Amazon ECS tasks, Kubernetes pods, or Amazon EC2 instances, for a virtual node beyond 10, Envoy container health checks for new and currently running Envoys begin to fail. Downstream applications sending traffic to the virtual node begin seeing request failures with HTTP status code 503.
Resolution
App Mesh's default quota for the number of Envoys per virtual node is 10. When the number of running Envoys exceeds this quota, new and currently running Envoys fail to connect to App Mesh's Envoy management service with gRPC status code 8 (RESOURCE_EXHAUSTED). This quota can be raised. For more information, see App Mesh service quotas.
If your issue is still not resolved, then consider opening a GitHub issue or contact AWS Support.
Requests fail with 503 when a virtual service backend horizontally scales out or in
Symptoms
When a backend virtual service is horizontally scaled out or in, requests from downstream applications fail with an HTTP 503 status code.
Resolution
App Mesh recommends several approaches to mitigate failure cases while scaling applications horizontally. For detailed information about how to prevent these failures, see App Mesh best practices.
If your issue is still not resolved, then consider opening a GitHub issue or contact AWS Support.
Envoy container crashes with segfault under increased load
Symptoms
Under a high traffic load, the Envoy proxy crashes due to a segmentation fault (Linux exit code 139). The Envoy process logs contain a statement like the following.
Caught Segmentation fault, suspect faulting address 0x0"
Resolution
The Envoy proxy has likely breached the operating system's default nofile ulimit, the limit on the number of files a process can have open at a time. This breach is due to the traffic causing more connections, which consume additional operating system sockets. To resolve this issue, increase the ulimit nofile value on the host operating system. If you are using Amazon ECS, this limit can be changed through the Ulimit settings on the task definition's resource limits settings.
If your issue is still not resolved, then consider opening a GitHub issue or contact AWS Support.