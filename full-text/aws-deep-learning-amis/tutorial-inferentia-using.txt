Using the DLAMI with AWS Neuron
A typical workflow with the AWS Neuron SDK is to compile a previously trained machine learning model on a compilation server. After this, distribute the artifacts to the Inf1 instances for execution. AWS Deep Learning AMI (DLAMI) comes pre-installed with everything you need to compile and run inference in an Inf1 instance that uses Inferentia. 
The following sections describe how to use the DLAMI with Inferentia. 
Topics
 Using TensorFlow-Neuron and the AWS Neuron Compiler
 Using AWS Neuron TensorFlow Serving
 Using MXNet-Neuron and the AWS Neuron Compiler
 Using MXNet-Neuron Model Serving
 Using PyTorch-Neuron and the AWS Neuron Compiler