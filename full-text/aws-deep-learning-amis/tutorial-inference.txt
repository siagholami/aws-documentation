Inference
This section provides tutorials on how to run inference using the DLAMI's frameworks and tools.
For tutorials using Elastic Inference, see Working with Amazon Elastic Inference 
Inference with Frameworks

Use Apache MXNet for Inference with an ONNX Model
Use Apache MXNet for Inference with a ResNet 50 Model
Use CNTK for Inference with an ONNX Model

Inference Tools

Model Server for Apache MXNet (MMS)
TensorFlow Serving
