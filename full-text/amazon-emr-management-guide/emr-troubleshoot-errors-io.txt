Input and Output Errors
The following errors are common in cluster input and output operations.
Topics
 Does your path to Amazon Simple Storage Service (Amazon S3) have at least three slashes?
 Are you trying to recursively traverse input directories?
 Does your output directory already exist?
 Are you trying to specify a resource using an HTTP URL?
 Are you referencing an Amazon S3 bucket using an invalid name format?
 Are you experiencing trouble loading data to or from Amazon S3?
Does your path to Amazon Simple Storage Service (Amazon S3) have at least three slashes?
When you specify an Amazon S3 bucket, you must include a terminating slash on the end of the URL. For example, instead of referencing a bucket as “s3n://aws-s3-bucket1”, you should use “s3n://aws-s3-bucket1/”, otherwise Hadoop fails your cluster in most cases. 
Are you trying to recursively traverse input directories?
Hadoop does not recursively search input directories for files. If you have a directory structure such as /corpus/01/01.txt, /corpus/01/02.txt, /corpus/02/01.txt, etc. and you specify /corpus/ as the input parameter to your cluster, Hadoop does not find any input files because the /corpus/ directory is empty and Hadoop does not check the contents of the subdirectories. Similarly, Hadoop does not recursively check the subdirectories of Amazon S3 buckets. 
The input files must be directly in the input directory or Amazon S3 bucket that you specify, not in subdirectories. 
Does your output directory already exist?
If you specify an output path that already exists, Hadoop will fail the cluster in most cases. This means that if you run a cluster one time and then run it again with exactly the same parameters, it will likely work the first time and then never again; after the first run, the output path exists and thus causes all successive runs to fail. 
Are you trying to specify a resource using an HTTP URL?
Hadoop does not accept resource locations specified using the http:// prefix. You cannot reference a resource using an HTTP URL. For example, passing in http://mysite/myjar.jar as the JAR parameter causes the cluster to fail. 
Are you referencing an Amazon S3 bucket using an invalid name format?
If you attempt to use a bucket name such as “aws-s3-bucket1.1” with Amazon EMR, your cluster will fail because Amazon EMR requires that bucket names be valid RFC 2396 host names; the name cannot end with a number. In addition, because of the requirements of Hadoop, Amazon S3 bucket names used with Amazon EMR must contain only lowercase letters, numbers, periods (.), and hyphens (-). For more information about how to format Amazon S3 bucket names, see Bucket Restrictions and Limitations in the Amazon Simple Storage Service Developer Guide. 
Are you experiencing trouble loading data to or from Amazon S3?
Amazon S3 is the most popular input and output source for Amazon EMR. A common mistake is to treat Amazon S3 as you would a typical file system. There are differences between Amazon S3 and a file system that you need to take into account when running your cluster. 
  If an internal error occurs in Amazon S3, your application needs to handle this gracefully and re-try the operation. 
  If calls to Amazon S3 take too long to return, your application may need to reduce the frequency at which it calls Amazon S3. 
  Listing all the objects in an Amazon S3 bucket is an expensive call. Your application should minimize the number of times it does this. 
There are several ways you can improve how your cluster interacts with Amazon S3. 
  Launch your cluster using the most recent release version of Amazon EMR. 
 Use S3DistCp to move objects in and out of Amazon S3. S3DistCp implements error handling, retries and back-offs to match the requirements of Amazon S3. For more information, see Distributed Copy Using S3DistCp. 
  Design your application with eventual consistency in mind. Use HDFS for intermediate data storage while the cluster is running and Amazon S3 only to input the initial data and output the final results. 
  If your clusters will commit 200 or more transactions per second to Amazon S3, contact support to prepare your bucket for greater transactions per second and consider using the key partition strategies described in Amazon S3 Performance Tips & Tricks. 
  Set the Hadoop configuration setting io.file.buffer.size to 65536. This causes Hadoop to spend less time seeking through Amazon S3 objects. 
  Consider disabling Hadoop's speculative execution feature if your cluster is experiencing Amazon S3 concurrency issues. This is also useful when you are troubleshooting a slow cluster. You do this by setting the mapreduce.map.speculative and mapreduce.reduce.speculative properties to false. When you launch a cluster, you can set these values using the mapred-env configuration classification. For more information, see Configuring Applications in the Amazon EMR Release Guide. 
  If you are running a Hive cluster, see Are you having trouble loading data to or from Amazon S3 into Hive?. 
For additional information, see Amazon S3 Error Best Practices in the Amazon Simple Storage Service Developer Guide. 